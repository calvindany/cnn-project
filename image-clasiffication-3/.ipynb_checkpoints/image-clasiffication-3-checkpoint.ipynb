{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ee3a607-3b13-44d6-8811-db4373eaccef",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0e1e3371-1a47-4ca1-9cb8-115bbd9db55d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cda3ae7a-f174-433d-863b-b58921e2c635",
   "metadata": {},
   "source": [
    "### Initialize Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a19b51a-3017-4169-83e3-656aee180ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = 'C:\\\\Users\\\\CALVIN\\\\Documents\\\\Datasets\\\\Batik Dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "69890625-8d23-419e-b692-ed624db81d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step will append image and labels\n",
    "images = []\n",
    "labels = [0, 1]\n",
    "for label in os.listdir(dataset_path):\n",
    "    label_dir = os.path.join(dataset_path, label)\n",
    "    \n",
    "    for image_file in os.listdir(label_dir):\n",
    "        image_path = os.path.join(label_dir, image_file)\n",
    "\n",
    "        try:\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            image = cv2.resize(image, (20, 20))\n",
    "        except:\n",
    "            continue\n",
    "        images.append(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b5f7e9-37b5-423a-8af7-8645b3e56449",
   "metadata": {},
   "source": [
    "### Converts Images and Label\n",
    "This steps will converts images and labels array to Numpy array\n",
    "\n",
    "For additional this steps also normalize numpy images array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "74c79207-5d7c-4fd9-a32a-3f52bbba24b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = np.array(images)\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "e34dd84a-ea0d-4543-b55e-8ef3562f08ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 20, 20)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "8ba1e0c7-1dcf-46b0-90a1-73262d5e266d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(977, 20, 20, 1)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images = images / 255\n",
    "images_reshaped = images.reshape(-1, 20, 20, 1)\n",
    "images_reshaped.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "513e5e42-5827-4ed8-9382-ac9992cbf5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array(['Batik', 'Bukan Batik'], dtype='<U11')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(type(images_reshaped))\n",
    "print(type(labels))\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "58d83551-73b6-426d-8cfd-be9b69fa4914",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid dtype: str352",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[78], line 14\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[38;5;241m=\u001b[39m keras\u001b[38;5;241m.\u001b[39mSequential([\n\u001b[0;32m      2\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mConv2D(filters\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, kernel_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m), activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, input_shape\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m1\u001b[39m)),\n\u001b[0;32m      3\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mMaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     keras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m20\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msoftmax\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m     11\u001b[0m ])\n\u001b[0;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(optimizer\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124madam\u001b[39m\u001b[38;5;124m'\u001b[39m, loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparse_categorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m, metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m---> 14\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_reshaped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:123\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tree\\__init__.py:428\u001b[0m, in \u001b[0;36mmap_structure\u001b[1;34m(func, *structures, **kwargs)\u001b[0m\n\u001b[0;32m    425\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m other \u001b[38;5;129;01min\u001b[39;00m structures[\u001b[38;5;241m1\u001b[39m:]:\n\u001b[0;32m    426\u001b[0m   assert_same_structure(structures[\u001b[38;5;241m0\u001b[39m], other, check_types\u001b[38;5;241m=\u001b[39mcheck_types)\n\u001b[0;32m    427\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unflatten_as(structures[\u001b[38;5;241m0\u001b[39m],\n\u001b[1;32m--> 428\u001b[0m                     [func(\u001b[38;5;241m*\u001b[39margs) \u001b[38;5;28;01mfor\u001b[39;00m args \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(flatten, structures))])\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid dtype: str352"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential([\n",
    "    keras.layers.Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(20, 20, 1)),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
    "    keras.layers.MaxPooling2D((2, 2)),\n",
    "\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(64, activation='relu'),\n",
    "    keras.layers.Dense(20, activation='softmax'),\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(images_reshaped, labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2754b77-863e-421b-b544-74af51827344",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
